#허깅페이스 코스 내용을 번역한 것
#가장 원리를 잘 설명하면서도, 쉽게 되어있으니 워드피스 토크나이저 알고리즘 이해는 이것만 보면 됨.
#BPE알고리즘 및 likelihood개념 선행지식 필요

1_ 훈련 알고리즘
BPE처럼 워드피스(WP)는 단일 알파벳과 모델에 사용되는 특수 토큰을 비롯한 소규모 사전으로부터 시작한다.
서브워드를 구별해주는 ##식별자를 더해주는 과정 때문에, 각 단어들은 각각 단어 내 모든 알파벳들에 ##이 붙으면서 쪼개진다.
예를 들어, word는 다음과 같이 쪼개진다.

w, ##o, ##r, ##d

식별자에 의해 쪼개지고 나면, 초기 알파벳은 어떤 단어의 맨 앞 글자를 나타내는 문자와 단어 내부에 있음을 나타내는 단어로 구성된다.
그리고나서 BPE처럼 워드피스는 머지(mearge) 룰을 학습한다. 가장 큰 차이는 머지할 글자페어를 고르는 방식이다.
WP는 빈도가 가장 높은 페어를 고르는 게 아닌, 다음의 공식으로 각 페어의 score를 계산한다.

 score = (페어의 빈도수)/첫 글자 빈도수*두 번째 글자 빈도수)
 
페어의 빈도수를 각 글자의 빈도수의 곱으로 나눔으로써, WP알고리즘은 개별 글자가 사전 내에서 더 적은 빈도수가 되도록 페어를 병합한다.
예를 들어, (un, ##able)은 페어가 사전 내에서 매우 자주 나오지만, 각각이 다른 단어에서 자주 사용되기 때문에  병합하지 않는다. 반면, (hu, ##gging)은 hu와, ##gging은 페어보다 각각 다른 곳에서 쓰이는 빈도가 더 적기 때무에 아마 빠르게 병합될 것으로 보인다.(그래도 hugging이라는 단어가 많이 등장한다는 가정은 있어야함.)
*역자 추가: 첫 글자, 두 번째 글자는 많이 나올 수록 score 낮아짐. 페어가 많을 수록 빈도수 높아짐. 다시 말하면 각각의 글자가 쌍에서만 쓰이면 score가 높고 쌍이 아닌 다른 곳에서 많이 쓰이면 score는 낮아짐.

BPE훈련의 예시로 같은 단어를 살펴보자.

("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)

먼저, 이 단어들은 다음과 같이 쪼개진다.

("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##g" "##s", 5)

그래서 초기 단어는 ["b", "h", "p", "##g", "##n", "##s", "##u"]가 될 것이다.(일단 특수토큰은 생각하지 않는다.)
가장 빈도가 높은 페어는 (##u, ##g)이다(20회). 그러나 ##u는 개별자로도 빈도수가 높고 그래서 score는 높지 않다.
(1/36점) *역자 추가: 1/36 = 20/(36*20)
##u가 들어간 모든 페어는 사실 다 같은 점수이다(1/36). 그래서 가장 높은 score 페어는 (##g, ##s)가 된다.
##u없이 만들어지는 가장 높은점수의 페어이므로, 가장 먼저 병합되는 페어는 (##g,##s) -->(##gs)이다.

우리가 병합할 때, 우리는 두 토큰 사이의 ##은 지운다는 것을 강조한다.
그래서 우리는 사전에 ##gs를 추가하고 나서 코퍼스 내 단어들에 머지룰을 적용한다.
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs"]
Corpus: ("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##gs", 5)

다음으로, ##u는 모든 가능한 페어에 다 들어있어서 다 같은 점수가 된다. 그래서 병합되는 페어는 (h,##u)->hu이다.
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu"]
Corpus: ("hu" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)

--편집중
